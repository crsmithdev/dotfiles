# [STRUCT] Design Document

## Vision Statement

Design structure output and annotation formats optimized for their specific purposes:
- **Output format**: Machine detection results with confidence for human review
- **Annotation format**: Sparse, authoritative ground truth for easy editing and comparison

Currently conflating both into a single format creates noise, confusion, and difficult workflows.

## Problem Statement

### Current Issues

1. **Output Pollution**: "other" sections and consecutive identical labels create noise
   ```yaml
   - [1, 16, other]      # low signal
   - [16, 24, other]     # duplicate
   - [24, drop]          # signal
   - [32, drop]          # possibly false positive
   ```

2. **Label Premature Exposure**: Including breakdown/buildup before they're accurate
   ```yaml
   - [81, 92, breakdown]    # confidence 0.6, not production-ready
   ```

3. **Polymorphic Format Confusion**: `[bar, label]` vs `[start, end, label]` mixing
   ```yaml
   - [1, 1, intro]          # span with zero duration (bug!)
   - [24, drop]             # event or span?
   - [24, 32, other]        # span
   ```

4. **Drop Semantics Ambiguity**: Events vs transitions vs implied moments
   ```yaml
   - [24, drop]             # just a moment marker
   # but musically: buildup (1-24) → drop → verse (25-80)
   ```

5. **Annotation Friction**: Editing polymorphic YAML directly is tedious
   ```yaml
   # Hard to edit, easy to make mistakes
   - [1, 24, intro]
   - [25, 80, other]         # what's this really?
   - [81, 92, breakdown]
   ```

6. **No Confidence Visibility**: Can't tell which detections are reliable during review
   ```yaml
   - [88, drop]             # confidence 0.6 or 0.9? Unknown!
   ```

7. **No Comparison Workflow**: Difficult to diff detected vs annotated structure

## Goals

1. **Clean Output**: Show only high-confidence detections, no noise
2. **Easy Annotations**: Human-friendly format for ground truth
3. **Clear Confidence**: See what the detector is uncertain about
4. **Efficient Workflow**: Convert detected → annotate → diff → evaluate
5. **Future-Proof**: Support progressive addition of labels
6. **Research-Friendly**: Keep low-confidence signals for investigation

## Solution Design

### Key Principles

**Principle 1: Separate Concerns**
- Output format optimized for machine→human review
- Annotation format optimized for human→machine ground truth
- Different use cases, different optimal formats

**Principle 2: Confidence-Based Filtering**
- Internally detect all labels with confidence scores
- Output filters by threshold (default: 0.7)
- Keep low-confidence signals in separate section
- No labels are "removed", just filtered for output

**Principle 3: Progressive Disclosure**
- Don't expose labels before they're ready
- Phase 1: Drops only (current capability)
- Phase 2: Add sections when accurate enough
- Phase 3: Full timeline annotation when mature

**Principle 4: Minimal Annotation Format**
- Make ground truth easy and unambiguous
- Human-readable ranges: `intro: 1-24`
- Support both explicit and implicit drop notation
- No confidence scores in annotation (implicitly 1.0)

**Principle 5: Merge Consecutive Identical Sections**
- Pure signal processing, no semantic change
- `[1,16,other]` + `[16,24,other]` → `[1,24,other]`
- Reduces noise without losing information

### Output Format v2

Purpose: Show what the detector found, with confidence for review

```yaml
# Track metadata
file: /home/user/music/track.flac
duration: 242.2
bpm: 128.0
downbeat: 0.02
time_signature: 4/4

# High-confidence detections (confidence >= 0.7)
detected_structure:
  # Spans with clear boundaries
  sections:
    - start: 1
      end: 24
      label: intro
      confidence: 0.9
      bars: 24

  # Event markers (drops, moments of interest)
  events:
    - bar: 24
      label: drop
      confidence: 0.9

    - bar: 105
      label: drop
      confidence: 0.8

# Medium-confidence detections (0.5 <= confidence < 0.7, optional)
detected_structure_experimental:
  sections:
    - start: 81
      end: 92
      label: breakdown
      confidence: 0.6

  events:
    - bar: 88
      label: drop
      confidence: 0.6
```

**Benefits:**
- Clear distinction: high-confidence (production) vs experimental (research)
- Confidence visible for decision-making
- Structured format (no polymorphism)
- Easy to diff against annotations
- Researchers can investigate low-confidence signals

### Annotation Format v2

Purpose: Ground truth for training/evaluation, easy to edit by hand

**Option A: List-of-objects (recommended, handles repetition)**
```yaml
file: track.flac
bpm: 128.0

structure:
  - label: intro
    bars: 1-24

  - label: verse
    bars: 25-80

  - label: breakdown
    bars: 81-92

  - label: buildup
    bars: 93-104

  - label: verse        # second verse - no key conflict!
    bars: 105-126

  - label: outro
    bars: 127-130

drops:
  - 24
  - 105
```

**Option B: Labeled sections (simpler for single instances)**
```yaml
file: track.flac
bpm: 128.0

structure:
  intro: 1-24
  verse: 25-80
  breakdown: 81-92
  buildup: 93-104
  outro: 127-130

drops:
  - 24
  - 105
```

**Key Difference:**
- **Option A** handles multiple verses/breakdowns/etc naturally
- **Option B** works only when each label appears once

**Recommendation**: Use Option A as canonical format. Tools accept both for convenience.

**Benefits:**
- Extremely readable and easy to edit
- No metadata noise
- Supports both explicit and implicit drops
- Can annotate just drops if full structure is unknown
- Range format matches DJ software conventions
- Handles multiple instances of same label naturally

### Workflow: From Detection to Ground Truth

**Step 1: Analyze Track**
```bash
$ edm analyze track.flac --output track_detected.yaml
```
Output: Machine detection with confidence visible

**Step 2: Convert to Annotation Template** (optional)
```bash
$ edm convert track_detected.yaml --to-annotation > track_annotated.yaml
```
Output: Human-friendly format, ready to edit

**Step 3: Human Reviews and Edits**
Open `track_annotated.yaml` in editor, make corrections

**Step 4: Compare Changes**
```bash
$ edm diff track_detected.yaml track_annotated.yaml
```

**Step 5: Evaluate**
```bash
$ edm evaluate structure --reference track_annotated.yaml
```

### Label Maturity Levels

Control what's exposed in output based on detection confidence:

**Phase 1: Drops Only** (current capability)
```python
output_config = {
    "include_labels": {"drop", "intro", "outro"},
    "exclude_labels": {"other", "breakdown", "buildup", "verse"},
    "min_confidence": 0.7,
}
```

**Phase 2: Add Section Transitions** (when breakdown/buildup accuracy improves)
```python
output_config = {
    "include_labels": {"drop", "intro", "outro", "breakdown", "buildup"},
    "exclude_labels": {"other", "verse"},
    "min_confidence": 0.7,
}
```

**Phase 3: Full Structure** (when all labels production-ready)
```python
output_config = {
    "include_labels": None,  # All labels
    "exclude_labels": {"other"},  # Only exclude truly unknown
    "min_confidence": 0.7,
}
```

### Implementation Approach

#### Option A: Parallel Formats (Recommended)

Keep existing format for backward compatibility, add new formats alongside:
- `track_detected.yaml` ← Current format
- `track_detected_v2.yaml` ← New format with confidence + sections
- `track_annotated.yaml` ← Human-friendly annotation format

**Benefits:**
- Zero breaking changes
- Existing workflows unaffected
- Can migrate gradually
- Compare old vs new formats side-by-side

**Tradeoffs:**
- More formats to maintain
- Potential for confusion

#### Option B: Format Migration (Breaking)

Migrate to new formats as default, deprecate old:
- `track_detected.yaml` ← New unified format
- `track_annotated.yaml` ← New human-friendly format

**Benefits:**
- Single, clear format
- Cleaner codebase
- Better for new users

**Tradeoffs:**
- Breaking change
- Existing scripts need updates

**Recommendation**: Start with Option A (parallel), migrate to Option B after stabilization.

## Conversion Rules

### Detected → Annotation

```python
def convert_detected_to_annotation(detected_dict):
    """Convert detector output to annotation template."""

    annotation = {
        "file": detected_dict["file"],
        "bpm": detected_dict["bpm"],
        "structure": [],
        "drops": []
    }

    # Convert sections to list format
    for section in detected_dict.get("detected_structure", {}).get("sections", []):
        annotation["structure"].append({
            "label": section["label"],
            "bars": f"{section['start']}-{section['end']}"
        })

    # Extract drop events
    for event in detected_dict.get("detected_structure", {}).get("events", []):
        if event["label"] == "drop":
            annotation["drops"].append(event["bar"])

    return annotation
```

### Annotation → Detected (for comparison)

```python
def convert_annotation_to_comparable(annotation_dict):
    """Convert annotation back to detected format for comparison."""

    result = {
        "detected_structure": {
            "sections": [],
            "events": []
        }
    }

    # Convert sections back to bar format
    if isinstance(annotation_dict.get("structure"), list):
        for item in annotation_dict["structure"]:
            start, end = map(int, item["bars"].split("-"))
            result["detected_structure"]["sections"].append({
                "start": start,
                "end": end,
                "label": item["label"],
                "confidence": 1.0  # Ground truth
            })
    else:
        for label, bars_str in annotation_dict.get("structure", {}).items():
            start, end = map(int, bars_str.split("-"))
            result["detected_structure"]["sections"].append({
                "start": start,
                "end": end,
                "label": label,
                "confidence": 1.0
            })

    # Add explicit drops
    for bar in annotation_dict.get("drops", []):
        result["detected_structure"]["events"].append({
            "bar": bar,
            "label": "drop",
            "confidence": 1.0  # Ground truth
        })

    return result
```

## Success Criteria

1. ✅ Output is clean (no "other" noise by default)
2. ✅ Annotations are easy to edit (human-readable format)
3. ✅ Confidence is visible when debugging
4. ✅ Workflow supports iteration (detect → annotate → diff → evaluate)
5. ✅ Future-proof (can add labels without breaking format)
6. ✅ No information loss (low-confidence signals available for research)
7. ✅ Easy comparison (detected vs annotated diffs are readable)

## Open Questions

1. Format choice for implementation: Option A (parallel formats) or Option B (migration)?
2. Confidence thresholds: Default 0.7, or different per label?
3. Phase 1 labels: Just drops, or include intro/outro?
4. Implicit drops: Support from day 1, or after section detection improves?
5. Research section: Always include experimental detections, or only on request?
6. CLI helpers: Want `edm convert`, `edm diff` commands, or just format docs?
